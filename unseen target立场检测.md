* 跑作者源代码能发现不少问题，有些细节不会在论文里提及
* 本实验的模型很依赖调参，可以用坐标下降法
* 本实验中，对Bi-LSTM的结果concat后，只作线性变换是不够的，需要有激活函数（易得该函数的作用不能被两个LSTM中的激活函数替代）
* 本实验中batch size很重要，需要仔细调，很小的变化就导致实验结果很不一样（原因不明）
* word2vec能带来较大提升，词向量是否在训练时更新，各有利弊，需要做实验确定（在本实验中，词向量的训练语料也是来自twitter，用同样的关键词抓下来，所以训练时不更新词向量似乎更合理，至于实际结果和训练时更新相当（略差0.几个点），原因不明）
* 本实验中word cutoff设为4时效果最好（word2vec模型文件中的值为5），也许因为此时的OOV和设为5时差不多，而又不会让一些在词表中的词成为unknown
* 本实验中dropout设为0.5时效果最好
* 本实验中优化器为ADAM，学习速率为1e-4，1e-3时会陷入更差的局部最优解
* 本实验没有用window层，也许因为Bi-LSTM本身就能学习上下文信息，参数还更少
* 本实验没用pooling层，也许因为LSTM的长时记忆能力确实牛逼，对句子的分类不需要pooling层
